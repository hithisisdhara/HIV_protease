{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from utilities import *\n",
    "\n",
    "########################################################################\n",
    "## helper functions\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "def read_edges(edgefile):\n",
    "    f = open(edgefile)\n",
    "    ans = []\n",
    "    for line in f:\n",
    "        line = tuple(sorted(map(lambda x: int(x), line.strip().split(\"\\t\"))))\n",
    "        ans.append(line)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_nodes(edges):\n",
    "    nodes = set()\n",
    "    for e in edges:\n",
    "        nodes.update(e)\n",
    "    return sorted(nodes)\n",
    "\n",
    "\n",
    "def get_inhs(nodes, df):\n",
    "    seqs = []\n",
    "    inh_vals = []\n",
    "    nodeptr = 0\n",
    "    l = len(nodes)\n",
    "    for line in df:\n",
    "        line = line.split('\\t')\n",
    "        node_ind = int(line[0])  # data specific, as I know thatthte first line is ind\n",
    "        if nodeptr < l and node_ind == nodes[nodeptr]:\n",
    "            nodeptr += 1\n",
    "            inh_vals.append(float(line[inh_ind]))\n",
    "    assert l == len(inh_vals)\n",
    "    df.close()\n",
    "    return inh_vals\n",
    "\n",
    "\n",
    "def binarize(p):\n",
    "    return [node_inhclass[n] for n in p]\n",
    "\n",
    "\n",
    "def last_continuous_ind(p, num):\n",
    "    ptr = 0\n",
    "    while ptr < len(p) and p[ptr] == num:\n",
    "        ptr += 1\n",
    "    return ptr - 1\n",
    "\n",
    "\n",
    "def first_ind(p, num):\n",
    "    return p.index(num)\n",
    "\n",
    "\n",
    "def monotone_increase(p):\n",
    "    return all([p[i] >= p[i - 1] for i in range(1, len(p))])\n",
    "\n",
    "\n",
    "def monotone_decrease(p):\n",
    "    return all([p[i] <= p[i - 1] for i in range(1, len(p))])\n",
    "\n",
    "\n",
    "def percent(x, t):\n",
    "    ans = x * 100.0 / t if t != 0 else 0\n",
    "    return \"{0:.2f}\".format(ans)\n",
    "\n",
    "\n",
    "def analyze(d):\n",
    "    total = 0\n",
    "    totals = defaultdict(int)\n",
    "    saperates = []\n",
    "    monotones = [\"inc\", \"dec\", \"monotone\"]\n",
    "    for k, v in d.items():\n",
    "        v = int(v)\n",
    "        tokens = k.split(\"_\")\n",
    "        num, meaning = int(tokens[0]), tokens[-1]\n",
    "        key = \"_\".join(tokens[1:])\n",
    "        while len(saperates) < num + 1:\n",
    "            saperates.append(defaultdict(int))\n",
    "        saperates[num][key] = v\n",
    "        if meaning not in monotones:\n",
    "            total += v\n",
    "        else:\n",
    "            totals[num] += v\n",
    "    assert sum(totals.values()) == total\n",
    "    print(inh, \" total paths analyzed:\", total)\n",
    "    all_keys = [\"inc\", \"dec\", \"not_monotone\", 'all_zeros', 'all_ones', \"zero_one\", \"one_zero\", \"spikes\"]\n",
    "    df = pd.DataFrame(columns=all_keys + [\"total\"], index=range(max(totals.keys()) + 1))\n",
    "\n",
    "    for i in range(max(totals.keys()) + 1):\n",
    "        d = saperates[i]\n",
    "        t = totals[i]\n",
    "        vals = [percent(d[k], t) for k in all_keys] + [t]\n",
    "        df.iloc[i] = vals\n",
    "    print(df)\n",
    "\n",
    "def var(s):\n",
    "    return round(np.var(s),4)\n",
    "def mean(s):\n",
    "    return round(np.mean(s),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inh = 'ATV'\n",
    "#vecstring = 'dist_vec'\n",
    "vecstring = 'count_vec'\n",
    "num_nbhrs = 400\n",
    "resistance_level = 3\n",
    "root_threshold = 2  # Threshold for which nodes wihh <= this number of mutations are selected for spanning trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecname = get_vecname(vecstring)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "########################################################################\n",
    "## global variables\n",
    "#######################################################################\n",
    "\n",
    "path = '/home/dshah8/Documents/Summer19/Harrison/data_ten_chunks/'  # where split data lies\n",
    "vecname = 'distvec' if vecstring == 'dist_vec' else 'countvec'\n",
    "ff = vecname + \"_data/\"\n",
    "folder = ff + str(\n",
    "    inh) + '_random_' + vecname + '_spanning_trees/'  # where spanning trees for the splits life, filtered by inhibitor != NA\n",
    "spfolder = str(inh) + '_random_shortest_paths/'  # where we will be storing shortest paths\n",
    "root_threshold = 2  # Threshold for which nodes wihh <= this number of mutations are selected for spanning trees\n",
    "\n",
    "\n",
    "def get_split_data_file(splitnum):\n",
    "    return path + 'PI_DataSet_6_19_random_split_' + str(splitnum) + '.txt'\n",
    "\n",
    "def get_shortest_path_folder():\n",
    "    return path + folder + spfolder\n",
    "\n",
    "\n",
    "def get_shortest_paths_file(splitnum):\n",
    "    edgefile = get_spanning_trees_file(splitnum)\n",
    "    spfile = edgefile.split(\".\")[0].split(\"/\")[-1] + \"_upto\" + str(\n",
    "        root_threshold) + \"mutsroots_shortestpaths_to_leaves.txt\"\n",
    "    return  get_shortest_path_folder() + spfile\n",
    "\n",
    "\n",
    "def get_spanning_trees_file(splitnum):\n",
    "    return path + folder + 'PI_DataSet_6_19_random_split_' + str(splitnum) + '_' + str(\n",
    "        num_nbhrs - 1) + 'nn_dist_vec_' + str(\n",
    "        inh) + 'filtered_spanning_tree_edges.txt'\n",
    "\n",
    "def get_writefilename():\n",
    "    return \"_\".join([get_shortest_path_folder(),'all',vecname,inh,\"random_shortest_paths_stats.txt\"])\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "writefilename = get_shortest_path_folder(inh,vecstring)+'meanpysparkdata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"path_id\",\"inh_type\",\"vec_type\",\"root_mutation\",\"path_length\",\"path_type\",\"fraction_above_inh_threshold\",\"path_variance\",\"path_mean\"]\n",
    "header = \",\".join(cols)\n",
    "wf = open(writefilename,\"w\")\n",
    "wf.write(header+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_csv(l):\n",
    "    return \",\".join([str(x) for x in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_id\n",
      "inh_type\n",
      "vec_type\n",
      "root_mutation\n",
      "path_length\n",
      "path_type\n",
      "fraction_above_inh_threshold\n",
      "path_variance\n",
      "path_mean\n"
     ]
    }
   ],
   "source": [
    "for c in cols:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATV  split  0\n",
      "countvec\n",
      "mutation: 1\n",
      "mutation,count 2\n",
      "47.422598361968994\n",
      "ATV  split  1\n",
      "countvec\n",
      "mutation: 1\n",
      "mutation,count 2\n",
      "54.91530632972717\n",
      "ATV  split  2\n",
      "countvec\n",
      "mutation: 1\n",
      "mutation,count 2\n",
      "66.09323763847351\n",
      "ATV  split  3\n",
      "countvec\n",
      "mutation: 0\n",
      "mutation: 1\n",
      "mutation,count 2\n",
      "69.38618993759155\n",
      "ATV  split  4\n",
      "countvec\n",
      "mutation: 1\n",
      "mutation,count 2\n",
      "51.54899597167969\n",
      "ATV  split  5\n",
      "countvec\n",
      "mutation: 0\n",
      "mutation: 1\n",
      "mutation,count 2\n",
      "72.0350890159607\n",
      "ATV  split  6\n",
      "countvec\n",
      "mutation: 1\n",
      "mutation,count 2\n",
      "82.82484555244446\n",
      "ATV  split  7\n",
      "countvec\n",
      "mutation: 0\n",
      "mutation: 1\n",
      "mutation,count 2\n",
      "72.85867881774902\n",
      "ATV  split  8\n",
      "countvec\n",
      "mutation: 0\n",
      "mutation: 1\n",
      "mutation,count 2\n",
      "81.03636384010315\n",
      "ATV  split  9\n",
      "countvec\n",
      "mutation: 0\n",
      "mutation: 1\n",
      "mutation,count 2\n",
      "55.20367884635925\n"
     ]
    }
   ],
   "source": [
    "path_id = -1\n",
    "inh_type = inh\n",
    "vec_type = vecname[0]\n",
    "for splitnum in range(10):\n",
    "    t0 = time.time()\n",
    "    print(str(inh), \" split \", str(splitnum))\n",
    "    datafile = get_split_data_file(splitnum)\n",
    "    df = open(datafile)\n",
    "    header = next(df).strip().split('\\t')\n",
    "    inh_ind = header.index(inh)\n",
    "    seq_start = header.index('P1')\n",
    "    \n",
    "\n",
    "    edgefile = get_spanning_trees_file(splitnum, inh, vecstring)\n",
    "    edges = read_edges(edgefile)\n",
    "    nodes = get_nodes(edges)\n",
    "    inh_vals = get_inhs(nodes, df)\n",
    "    node_ind = {str(v): i for i, v in enumerate(nodes)}\n",
    "    node_inhclass = {str(n): int(inh_vals[node_ind[str(n)]] > resistance_level) for n in nodes}\n",
    "\n",
    "    spfile = get_shortest_paths_file(splitnum, inh, vecstring)\n",
    "    print(vecname)\n",
    "    f = open(spfile)\n",
    "    mutation = ''\n",
    "    for line in f:\n",
    "        if \"mutation\" in line:\n",
    "            if mutation:\n",
    "                print('mutation:', mutation)\n",
    "            mutation = str(line.strip().split(\":\")[1])\n",
    "        else:\n",
    "            path_id += 1\n",
    "            root_mutation = mutation\n",
    "            line = line.strip().split(\",\")\n",
    "            # find if the given seq is monotone in inh vals\n",
    "            vals = [inh_vals[node_ind[n]] for n in line]\n",
    "            binary = binarize(line)\n",
    "            path_length = len(line)\n",
    "            s = sum(binary)\n",
    "            fraction_above_inh_threshold = round(float(s)/path_length,4)\n",
    "            path_variance = var(vals)\n",
    "            path_mean = mean(vals)\n",
    "            if s == path_length:\n",
    "                path_type = \"above\"\n",
    "                  \n",
    "            elif s == 0:\n",
    "                path_type = \"below\"\n",
    "                \n",
    "            else:\n",
    "                last_cont_zero = last_continuous_ind(binary, 0)\n",
    "                first_one = first_ind(binary, 1)\n",
    "                last_cont_one = last_continuous_ind(binary, 1)\n",
    "                first_zero = first_ind(binary, 0)\n",
    "                \n",
    "                if last_cont_zero != -1 and last_cont_zero + 1 == first_one and s == len(binary[first_one:]):\n",
    "                    path_type = \"gains\"\n",
    "                    path_variance = var(vals[first_one:])\n",
    "                    path_mean = mean(vals[first_one:])\n",
    "                    \n",
    "                elif last_cont_one != 1 and last_cont_one + 1 == first_zero and s == len(binary[:first_zero]):\n",
    "                    path_type = \"looses\"\n",
    "                    path_variance = var(vals[:first_zero])\n",
    "                    path_mean = mean(vals[:first_zero])\n",
    "                else:\n",
    "                    path_type = 'spikes'\n",
    "                    path_variance = var([v for v in vals if v > 3.0])\n",
    "                    path_mean = mean([v for v in vals if v > 3.0])\n",
    "            token = write_line([path_id,inh_type,vec_type,root_mutation,path_length,path_type,fraction_above_inh_threshold,path_variance, path_mean],wf)\n",
    "    print('mutation,count', mutation)\n",
    "    f.close()\n",
    "    print(time.time() - t0)\n",
    "    t0 = time.time()\n",
    "wf.close()\n",
    "\n",
    "#analyze(inh_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecname"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fraction_above_inh_threshold,s,last_cont_one "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path_type, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telly_count = -2\n",
    "rf = open(writefilename)\n",
    "for _ in rf:\n",
    "    telly_count+=1\n",
    "rf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert telly_count == path_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
